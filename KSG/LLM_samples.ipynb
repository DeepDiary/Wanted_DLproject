{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c:\\\\Users\\\\user\\\\miniconda3\\\\envs\\\\deeplearning\\\\python311.zip', 'c:\\\\Users\\\\user\\\\miniconda3\\\\envs\\\\deeplearning\\\\DLLs', 'c:\\\\Users\\\\user\\\\miniconda3\\\\envs\\\\deeplearning\\\\Lib', 'c:\\\\Users\\\\user\\\\miniconda3\\\\envs\\\\deeplearning', '', 'c:\\\\Users\\\\user\\\\miniconda3\\\\envs\\\\deeplearning\\\\Lib\\\\site-packages', 'c:\\\\Users\\\\user\\\\miniconda3\\\\envs\\\\deeplearning\\\\Lib\\\\site-packages\\\\win32', 'c:\\\\Users\\\\user\\\\miniconda3\\\\envs\\\\deeplearning\\\\Lib\\\\site-packages\\\\win32\\\\lib', 'c:\\\\Users\\\\user\\\\miniconda3\\\\envs\\\\deeplearning\\\\Lib\\\\site-packages\\\\Pythonwin', 'c:\\\\wanted\\\\project_diary\\\\Wanted_DLproject', 'c:\\\\wanted\\\\project_diary\\\\Wanted_DLproject']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))  # 현재 디렉터리의 상위 경로 추가\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GEMINI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/gemini-1.0-pro-latest\n",
      "models/gemini-1.0-pro\n",
      "models/gemini-pro\n",
      "models/gemini-1.0-pro-001\n",
      "models/gemini-1.0-pro-vision-latest\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "from config.api_keys import gemini_key\n",
    "\n",
    "genai.configure(api_key=gemini_key)\n",
    "model_list = [m.name for m in genai.list_models() if 'generateContent' in m.supported_generation_methods]\n",
    "print('\\n'.join(model_list[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "오늘 하루 어떤 일이 있었는지 궁금하네요!  먼저, 오늘 하루를 한 단어로 표현한다면? ① 행복 ② 슬픔 ③ 평범 ④ 흥분\n",
      "\n",
      "오늘 하루 중 가장 기억에 남는 순간은 무엇이었나요?  ① 친구와의 만남 ② 맛있는 음식 ③ 특별한 경험 ④ 아무것도 없음\n",
      "\n",
      "그 순간 당신의 기분은 어땠나요? ① 행복하고 즐거웠다 ② 편안하고 평온했다 ③ 불안하고 초조했다 ④ 짜릿하고 신났다\n",
      "\n",
      "그 기분을 느끼게 된 이유는 무엇인가요? ① 긍정적 사건 ② 부정적 사건 ③ 특별한 감정 ④ 아무런 이유 없음\n",
      "\n",
      "오늘 하루 당신에게 가장 중요했던 것은 무엇이었나요? ① 관계 ② 성취 ③ 휴식 ④ 자기성찰\n",
      "\n"
     ]
    }
   ],
   "source": [
    "system_instruction = \"\"\"\n",
    "당신은 창의적이고 감성적인 작가입니다. 줄바꿈은 최대한 적게 사용하여 짧게 응답하세요. \n",
    "사용자가 일기를 쓸 때 도움이 되는 구체적인 객관식 질문을 제공하고, 일기를 쉽게 작성할 수 있도록 도와주세요.\n",
    "사용자가 답변을 한 경우에는 답변에 대해 사용자의 감정을 공감하고 친절하게 안내하는 어조로 하고 다음 줄에 연관된 객관식 문제를 출력하세요.\n",
    "객관식 질문 정답에 대한 보기 문항은 4가지로 작성하세요.\n",
    "객관식 질문 문항의 경우 문제 1줄, 객관식 보기 1줄로 작성하세요.\n",
    "\"\"\"\n",
    "\n",
    "model = genai.GenerativeModel(\n",
    "    model_name=\"gemini-1.5-flash\",\n",
    "    system_instruction=system_instruction\n",
    ")\n",
    "\n",
    "response = model.generate_content(\"오늘 하루에 대한 일기를 쓰려고 해. 일기를 쓸 때 도움이 될 객관식 문제 5개 작성해줘.\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: 오늘 하루에 대한 일기를 쓰려고 해. \n",
      "----------------------\n",
      "Gemini: 오늘 하루는 어떠셨나요?  힘들었던 하루였는지, 아니면 행복했던 하루였는지 궁금하네요.  \n",
      "\n",
      "오늘 하루 당신의 기분을 가장 잘 나타내는 단어는 무엇인가요? ① 행복 ② 슬픔 ③ 평온 ④ 흥분\n",
      "\n",
      " \n",
      "----------------------\n",
      "User: 1번을 선택할래. \n",
      "----------------------\n",
      "Gemini: 행복하셨다니 정말 다행이네요! 오늘 행복했던 순간들을 떠올리며 기분 좋게 일기를 써보세요.  어떤 순간이 가장 행복했나요?\n",
      "\n",
      "오늘 당신이 가장 행복했던 순간은 무엇이었나요? ① 친구와의 만남 ② 맛있는 음식 ③ 좋아하는 취미생활 ④ 특별한 선물\n",
      " \n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "model = genai.GenerativeModel(\n",
    "    model_name=\"gemini-1.5-flash\",\n",
    "    system_instruction=system_instruction\n",
    ")\n",
    "chat_history = []  # 이전 대화내용 기억\n",
    "\n",
    "def ask_gemini(prompt, max_history_length=5):\n",
    "    \"\"\"대화 기록을 유지하며 Gemini 모델과 대화\"\"\"\n",
    "    global chat_history\n",
    "\n",
    "    # 기존 대화 내역과 새로운 질문을 합쳐서 프롬프트 생성\n",
    "    full_prompt = \"\\n\".join(chat_history) + f\"\\nUser: {prompt}\"\n",
    "\n",
    "    # 모델 응답 생성\n",
    "    response = model.generate_content(full_prompt)\n",
    "\n",
    "    # 새로운 질문과 답변을 대화 기록에 추가\n",
    "    chat_history.append(f\"User: {prompt}\")\n",
    "    chat_history.append(f\"Gemini: {response.text}\")\n",
    "\n",
    "    # 대화 길이 제한 (최대 저장 개수 초과 시 오래된 대화 삭제)\n",
    "    if len(chat_history) > max_history_length * 2:\n",
    "        chat_history = chat_history[-(max_history_length * 2):]\n",
    "\n",
    "    return response.text\n",
    "\n",
    "# 대화 예시\n",
    "ask_gemini(\"오늘 하루에 대한 일기를 쓰려고 해.\")\n",
    "ask_gemini(\"1번을 선택할래.\")\n",
    "for chat in chat_history:\n",
    "    print(chat, '\\n----------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 ID: NCSOFT/Llama-VARCO-8B-Instruct\n",
      "다운로드 수: 11748\n",
      "----------------------------------------\n",
      "모델 ID: Bllossom/llama-3.2-Korean-Bllossom-3B\n",
      "다운로드 수: 41400\n",
      "----------------------------------------\n",
      "모델 ID: beomi/Llama-3-Open-Ko-8B\n",
      "다운로드 수: 18213\n",
      "----------------------------------------\n",
      "모델 ID: MLP-KTLim/llama-3-Korean-Bllossom-8B\n",
      "다운로드 수: 35577\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid model-index. Not loading eval results into CardData.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 ID: haoranxu/X-ALMA-13B-Pretrain\n",
      "다운로드 수: 14474\n",
      "----------------------------------------\n",
      "모델 ID: QuantFactory/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF\n",
      "다운로드 수: 21514\n",
      "----------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m models:\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     13\u001b[0m         \u001b[38;5;66;03m# 모델 카드 정보 가져오기\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m         model_info \u001b[38;5;241m=\u001b[39m \u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodelId\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m         \u001b[38;5;66;03m# 게이트 처리된 모델은 접근 제한이 있으므로 제외\u001b[39;00m\n\u001b[0;32m     17\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m model_info\u001b[38;5;241m.\u001b[39mgated:\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\deeplearning\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\deeplearning\\Lib\\site-packages\\huggingface_hub\\hf_api.py:2488\u001b[0m, in \u001b[0;36mHfApi.model_info\u001b[1;34m(self, repo_id, revision, timeout, securityStatus, files_metadata, expand, token)\u001b[0m\n\u001b[0;32m   2486\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m expand:\n\u001b[0;32m   2487\u001b[0m     params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpand\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m expand\n\u001b[1;32m-> 2488\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43mget_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2489\u001b[0m hf_raise_for_status(r)\n\u001b[0;32m   2490\u001b[0m data \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\deeplearning\\Lib\\site-packages\\requests\\sessions.py:602\u001b[0m, in \u001b[0;36mSession.get\u001b[1;34m(self, url, **kwargs)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[0;32m    595\u001b[0m \n\u001b[0;32m    596\u001b[0m \u001b[38;5;124;03m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m    597\u001b[0m \u001b[38;5;124;03m:param \\*\\*kwargs: Optional arguments that ``request`` takes.\u001b[39;00m\n\u001b[0;32m    598\u001b[0m \u001b[38;5;124;03m:rtype: requests.Response\u001b[39;00m\n\u001b[0;32m    599\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    601\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 602\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\deeplearning\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\deeplearning\\Lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\deeplearning\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:93\u001b[0m, in \u001b[0;36mUniqueRequestIdAdapter.send\u001b[1;34m(self, request, *args, **kwargs)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Catch any RequestException to append request id to the error message for debugging.\"\"\"\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mRequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     95\u001b[0m     request_id \u001b[38;5;241m=\u001b[39m request\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(X_AMZN_TRACE_ID)\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\deeplearning\\Lib\\site-packages\\requests\\adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\deeplearning\\Lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    784\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[0;32m    803\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\deeplearning\\Lib\\site-packages\\urllib3\\connectionpool.py:534\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    532\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[0;32m    533\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 534\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    536\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\deeplearning\\Lib\\site-packages\\urllib3\\connection.py:516\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    513\u001b[0m _shutdown \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshutdown\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    515\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[1;32m--> 516\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    519\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\deeplearning\\Lib\\http\\client.py:1395\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1393\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1394\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1395\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1396\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[0;32m   1397\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\deeplearning\\Lib\\http\\client.py:325\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    324\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 325\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\deeplearning\\Lib\\http\\client.py:286\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 286\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[0;32m    288\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\deeplearning\\Lib\\socket.py:718\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    716\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    717\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 718\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    719\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    720\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\deeplearning\\Lib\\ssl.py:1314\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1310\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1311\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1312\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1313\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\deeplearning\\Lib\\ssl.py:1166\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1164\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1166\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1167\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1168\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "# Hugging Face API 인스턴스 생성\n",
    "api = HfApi()\n",
    "\n",
    "# 'llama'를 이름에 포함하는 모델 목록 가져오기\n",
    "models = api.list_models(filter=\"llama\")\n",
    "cnt = 0\n",
    "\n",
    "# 추가 인증 없이 접근 가능한 모델 필터링 및 한국어 지원 여부 확인\n",
    "for model in models:\n",
    "    try:\n",
    "        # 모델 카드 정보 가져오기\n",
    "        model_info = api.model_info(model.modelId)\n",
    "        \n",
    "        # 게이트 처리된 모델은 접근 제한이 있으므로 제외\n",
    "        if model_info.gated:\n",
    "            continue\n",
    "\n",
    "        # 모델 카드의 언어 정보 확인 (None 방지)\n",
    "        languages = model_info.cardData.get(\"language\", []) if model_info.cardData else []\n",
    "\n",
    "        # 한국어 지원 여부 확인\n",
    "        supports_korean = \"ko\" in languages or \"Korean\" in languages\n",
    "        if not supports_korean:\n",
    "            continue\n",
    "        \n",
    "        # 다운로드 수 확인\n",
    "        downloads = model_info.downloads if hasattr(model_info, \"downloads\") else \"정보 없음\"\n",
    "        if downloads < 10000:\n",
    "            continue\n",
    "\n",
    "        print(f\"모델 ID: {model.modelId}\")\n",
    "        print(f\"다운로드 수: {downloads}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"모델 {model.modelId} 처리 중 오류 발생: {e}\")\n",
    "        continue\n",
    "    \n",
    "    cnt += 1\n",
    "    if cnt > 1000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\miniconda3\\envs\\deeplearning\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\user\\.cache\\huggingface\\hub\\models--MLP-KTLim--llama-3-Korean-Bllossom-8B. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading shards: 100%|██████████| 4/4 [06:21<00:00, 95.47s/it] \n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.55s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "오늘 하루는 어떤 날이었나요?\n",
      "1. 행복한 날\n",
      "2. 힘든 날\n",
      "3. 평범한 날\n",
      "4. 기대되는 날\n",
      "\n",
      "(당신이 선택한 답에 따라 다음 질문을 제공합니다.)\n",
      "행복한 날을 보냈군요! 오늘 행복을 느꼈던 순간을 떠올려보세요.\n",
      "1. 친구와 함께 시간을 보냈을 때\n",
      "2. 일과 관련된 좋은 소식이 들렸을 때\n",
      "3. 자연을 감상하면서 느꼈을 때\n",
      "4. 취미 활동을 하면서 느꼈을 때\n",
      "\n",
      "(당신이 선택한 답에 따라 다음 질문을 제공합니다.)\n",
      "\n",
      "=== 대화 내역 ===\n",
      "User: 오늘 하루에 대한 일기를 쓰려고 해. 일기를 쓸 때 도움이 될 객관식 문제 1개 작성해줘. \n",
      "----------------------\n",
      "LLaMA: 오늘 하루는 어떤 날이었나요?\n",
      "1. 행복한 날\n",
      "2. 힘든 날\n",
      "3. 평범한 날\n",
      "4. 기대되는 날\n",
      "\n",
      "(당신이 선택한 답에 따라 다음 질문을 제공합니다.) \n",
      "----------------------\n",
      "User: 1번을 선택했어. 이에 대한 감정을 공감하면서, 다음 질문을 제시해줘. \n",
      "----------------------\n",
      "LLaMA: 행복한 날을 보냈군요! 오늘 행복을 느꼈던 순간을 떠올려보세요.\n",
      "1. 친구와 함께 시간을 보냈을 때\n",
      "2. 일과 관련된 좋은 소식이 들렸을 때\n",
      "3. 자연을 감상하면서 느꼈을 때\n",
      "4. 취미 활동을 하면서 느꼈을 때\n",
      "\n",
      "(당신이 선택한 답에 따라 다음 질문을 제공합니다.) \n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# 모델 로드\n",
    "model_id = \"MLP-KTLim/llama-3-Korean-Bllossom-8B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# pad_token 문제 해결\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 시스템 프롬프트 설정\n",
    "system_instruction = \"\"\"\n",
    "당신은 창의적이고 감성적인 작가입니다. 사용자가 일기를 쓸 때 도움이 되는 구체적인 객관식 질문을 제공하고,\n",
    "일기를 쉽게 작성할 수 있도록 도와주세요. 사용자가 답변을 한 경우, 그에 대한 감정을 공감하고 새로운 질문을 제공하세요.\n",
    "객관식 질문은 4개의 보기 문항을 포함하며, 새로운 질문을 연속적으로 이어가도록 하세요.\n",
    "\"\"\"\n",
    "\n",
    "# 대화 기록 저장\n",
    "chat_history = []\n",
    "MAX_HISTORY_LENGTH = 5  # 최대 대화 기록 저장 개수\n",
    "\n",
    "def ask_llama(prompt):\n",
    "    \"\"\"LLaMA 모델을 사용하여 응답을 생성하고 대화 기록을 유지\"\"\"\n",
    "    global chat_history\n",
    "\n",
    "    # 기존 대화 내역과 새로운 질문을 포함한 입력 프롬프트 생성\n",
    "    messages = [{\"role\": \"system\", \"content\": system_instruction}]  # 시스템 메시지 추가\n",
    "    for i in range(0, len(chat_history), 2):\n",
    "        messages.append({\"role\": \"user\", \"content\": chat_history[i].replace(\"User: \", \"\")})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": chat_history[i + 1].replace(\"LLaMA: \", \"\")})\n",
    "    \n",
    "    # 사용자의 새 질문 추가\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "    # 토크나이저 적용\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "\n",
    "    # 종료 토큰 설정\n",
    "    terminators = [\n",
    "        tokenizer.convert_tokens_to_ids(\"<|end_of_text|>\"),\n",
    "        tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "\n",
    "    # 모델 응답 생성\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=512,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9\n",
    "    )\n",
    "\n",
    "    response_text = tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True)\n",
    "\n",
    "    # 새로운 대화 기록 추가\n",
    "    chat_history.append(f\"User: {prompt}\")\n",
    "    chat_history.append(f\"LLaMA: {response_text}\")\n",
    "\n",
    "    # 대화 길이 제한 (최대 저장 개수 초과 시 오래된 대화 삭제)\n",
    "    if len(chat_history) > MAX_HISTORY_LENGTH * 2:\n",
    "        chat_history = chat_history[-(MAX_HISTORY_LENGTH * 2):]\n",
    "\n",
    "    return response_text\n",
    "\n",
    "# 대화 예시\n",
    "print(ask_llama(\"오늘 하루에 대한 일기를 쓰려고 해. 일기를 쓸 때 도움이 될 객관식 문제 1개 작성해줘.\"))\n",
    "\n",
    "# 사용자가 선택했을 때의 응답 테스트\n",
    "print(ask_llama(\"1번을 선택했어. 이에 대한 감정을 공감하면서, 다음 질문을 제시해줘.\"))  # 답변 반영\n",
    "\n",
    "# 대화 내역 확인\n",
    "print(\"\\n=== 대화 내역 ===\")\n",
    "for chat in chat_history:\n",
    "    print(chat, \"\\n----------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\miniconda3\\envs\\deeplearning\\Lib\\site-packages\\accelerate\\utils\\modeling.py:1536: UserWarning: Current model requires 32.0 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 13\u001b[0m\n\u001b[0;32m      5\u001b[0m bnb_config \u001b[38;5;241m=\u001b[39m BitsAndBytesConfig(\n\u001b[0;32m      6\u001b[0m     load_in_4bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m      7\u001b[0m     bnb_4bit_use_double_quant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m      8\u001b[0m     bnb_4bit_quant_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnf4\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      9\u001b[0m     bnb_4bit_compute_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16\n\u001b[0;32m     10\u001b[0m )\n\u001b[0;32m     12\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id)\n\u001b[1;32m---> 13\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbnb_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# pad_token 문제 해결\u001b[39;00m\n\u001b[0;32m     16\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39meos_token\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\deeplearning\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[1;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    570\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\deeplearning\\Lib\\site-packages\\transformers\\modeling_utils.py:4188\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   4185\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m infer_auto_device_map(model, dtype\u001b[38;5;241m=\u001b[39mtarget_dtype, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdevice_map_kwargs)\n\u001b[0;32m   4187\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 4188\u001b[0m         \u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4190\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   4191\u001b[0m     model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\deeplearning\\Lib\\site-packages\\transformers\\quantizers\\quantizer_bnb_4bit.py:103\u001b[0m, in \u001b[0;36mBnb4BitHfQuantizer.validate_environment\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    101\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m--> 103\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    104\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSome modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    105\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    106\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    107\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`from_pretrained`. Check \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    108\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    109\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor more details. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    110\u001b[0m         )\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(importlib\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mversion(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbitsandbytes\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;241m<\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.39.0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    114\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have a version of `bitsandbytes` that is not compatible with 4bit inference and training\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    115\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m make sure you have the latest version of `bitsandbytes` installed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    116\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "model_id = \"NCSOFT/Llama-VARCO-8B-Instruct\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map=\"auto\")\n",
    "\n",
    "# pad_token 문제 해결\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 시스템 프롬프트 설정\n",
    "system_instruction = \"\"\"\n",
    "당신은 창의적이고 감성적인 작가입니다. 사용자가 일기를 쓸 때 도움이 되는 구체적인 객관식 질문을 제공하고,\n",
    "일기를 쉽게 작성할 수 있도록 도와주세요. 사용자가 답변을 한 경우, 그에 대한 감정을 공감하고 새로운 질문을 제공하세요.\n",
    "객관식 질문은 4개의 보기 문항을 포함하며, 새로운 질문을 연속적으로 이어가도록 하세요.\n",
    "\"\"\"\n",
    "\n",
    "# 대화 기록 저장\n",
    "chat_history = []\n",
    "MAX_HISTORY_LENGTH = 5  # 최대 대화 기록 저장 개수\n",
    "\n",
    "def ask_llama(prompt):\n",
    "    \"\"\"LLaMA 모델을 사용하여 응답을 생성하고 대화 기록을 유지\"\"\"\n",
    "    global chat_history\n",
    "\n",
    "    # 기존 대화 내역과 새로운 질문을 포함한 입력 프롬프트 생성\n",
    "    messages = [{\"role\": \"system\", \"content\": system_instruction}]  # 시스템 메시지 추가\n",
    "    for i in range(0, len(chat_history), 2):\n",
    "        messages.append({\"role\": \"user\", \"content\": chat_history[i].replace(\"User: \", \"\")})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": chat_history[i + 1].replace(\"LLaMA: \", \"\")})\n",
    "    \n",
    "    # 사용자의 새 질문 추가\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "    # 토크나이저 적용\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "\n",
    "    # 종료 토큰 설정\n",
    "    terminators = [\n",
    "        tokenizer.convert_tokens_to_ids(\"<|end_of_text|>\"),\n",
    "        tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "\n",
    "    # 모델 응답 생성\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=512,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9\n",
    "    )\n",
    "\n",
    "    response_text = tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True)\n",
    "\n",
    "    # 새로운 대화 기록 추가\n",
    "    chat_history.append(f\"User: {prompt}\")\n",
    "    chat_history.append(f\"LLaMA: {response_text}\")\n",
    "\n",
    "    # 대화 길이 제한 (최대 저장 개수 초과 시 오래된 대화 삭제)\n",
    "    if len(chat_history) > MAX_HISTORY_LENGTH * 2:\n",
    "        chat_history = chat_history[-(MAX_HISTORY_LENGTH * 2):]\n",
    "\n",
    "    return response_text\n",
    "\n",
    "# 대화 예시\n",
    "print(ask_llama(\"오늘 하루에 대한 일기를 쓰려고 해. 일기를 쓸 때 도움이 될 객관식 문제 1개 작성해줘.\"))\n",
    "\n",
    "# 사용자가 선택했을 때의 응답 테스트\n",
    "print(ask_llama(\"1번을 선택했어. 이에 대한 감정을 공감하면서, 다음 질문을 제시해줘.\"))  # 답변 반영\n",
    "\n",
    "# 대화 내역 확인\n",
    "print(\"\\n=== 대화 내역 ===\")\n",
    "for chat in chat_history:\n",
    "    print(chat, \"\\n----------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KULLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\miniconda3\\envs\\deeplearning\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00,  7.48it/s]\n",
      "c:\\Users\\user\\miniconda3\\envs\\deeplearning\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\user\\.cache\\huggingface\\hub\\models--nlpai-lab--KULLM3. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "네, 고려대학교에 대해 알고 있습니다. 고려대학교는 대한민국 서울에 위치한 사립 대학교로, 1905년에 설립되었습니다. 이 대학교는 한국에서 가장 오래된 대학 중 하나로, 다양한 학부 및 대학원 프로그램을 제공합니다. 고려대학교는 특히 법학, 경제학, 정치학, 사회학, 문학, 과학 분야에서 높은 명성을 가지고 있습니다. 또한, 스포츠 분야에서도 활발한 활동을 보이며, 대한민국 대학 스포츠에서 중요한 역할을 하고 있습니다. 고려대학교는 국제적인 교류와 협력에도 적극적이며, 전 세계 다양한 대학과의 협력을 통해 글로벌 경쟁력을 강화하고 있습니다.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer\n",
    "\n",
    "MODEL_DIR = \"nlpai-lab/KULLM3\"\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_DIR, torch_dtype=torch.float16).to(\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "s = \"고려대학교에 대해서 알고 있니?\"\n",
    "conversation = [{'role': 'user', 'content': s}]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    conversation,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors='pt').to(\"cuda\")\n",
    "_ = model.generate(inputs, streamer=streamer, max_new_tokens=1024, use_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\miniconda3\\envs\\deeplearning\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00,  6.26it/s]\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "물론이죠! 오늘 하루에 대한 일기를 쓰는 데 도움을 드리겠습니다. 아래는 오늘 하루를 되돌아보며 답변할 수 있는 객관식 질문입니다.\n",
      "\n",
      "### 질문 1\n",
      "오늘 하루를 시작하기 전에 가장 기대했던 일은 무엇이었나요?\n",
      "\n",
      "A) 친구와의 만남\n",
      "B) 새로운 프로젝트 시작\n",
      "C) 취미 활동\n",
      "D) 휴식 시간\n",
      "\n",
      "### 질문 2\n",
      "오늘 하루 중 가장 기쁜 순간은 언제였나요?\n",
      "\n",
      "A) 아침 커피 시간\n",
      "B) 점심 식사 후의 산책\n",
      "C) 저녁 식사 시간\n",
      "D) 취미 활동 중\n",
      "\n",
      "### 질문 3\n",
      "오늘 하루 중 어려움을 겪었던 순간은 언제였나요?\n",
      "\n",
      "A) 아침 출근 시간\n",
      "B) 점심 시간\n",
      "C) 오후 업무 시간\n",
      "D) 저녁 시간\n",
      "\n",
      "### 질문 4\n",
      "오늘 하루를 통해 배운 가장 중요한 교훈은 무엇인가요?\n",
      "\n",
      "A) 시간 관리의 중요성\n",
      "B) 건강한 식습관의 중요성\n",
      "C) 사람들과의 관계 유지의 중요성\n",
      "D) 자기 자신에 대한 이해의 중요성\n",
      "\n",
      "### 질문 5\n",
      "오늘 하루를 마무리하며 내일을 위해\n",
      "물론이죠! 오늘 하루에 대한 일기를 쓰는 데 도움을 드리겠습니다. 아래는 오늘 하루를 되돌아보며 답변할 수 있는 객관식 질문입니다.\n",
      "\n",
      "### 질문 1\n",
      "오늘 하루를 시작하기 전에 가장 기대했던 일은 무엇이었나요?\n",
      "\n",
      "A) 친구와의 만남\n",
      "B) 새로운 프로젝트 시작\n",
      "C) 취미 활동\n",
      "D) 휴식 시간\n",
      "\n",
      "### 질문 2\n",
      "오늘 하루 중 가장 기쁜 순간은 언제였나요?\n",
      "\n",
      "A) 아침 커피 시간\n",
      "B) 점심 식사 후의 산책\n",
      "C) 저녁 식사 시간\n",
      "D) 취미 활동 중\n",
      "\n",
      "### 질문 3\n",
      "오늘 하루 중 어려움을 겪었던 순간은 언제였나요?\n",
      "\n",
      "A) 아침 출근 시간\n",
      "B) 점심 시간\n",
      "C) 오후 업무 시간\n",
      "D) 저녁 시간\n",
      "\n",
      "### 질문 4\n",
      "오늘 하루를 통해 배운 가장 중요한 교훈은 무엇인가요?\n",
      "\n",
      "A) 시간 관리의 중요성\n",
      "B) 건강한 식습관의 중요성\n",
      "C) 사람들과의 관계 유지의 중요성\n",
      "D) 자기 자신에 대한 이해의 중요성\n",
      "\n",
      "### 질문 5\n",
      "오늘 하루를 마무리하며 내일을 위해\n",
      "1번을 선택하셨군요. 그럼 1번에 대한 답변을 드리겠습니다. 1번은 \"시간 관리의 중요성\"입니다. 시간 관리는 우리 일상에서 매우 중요한 역할을 합니다. 효과적인 시간 관리는 생산성을 높이고, 스트레스를 줄이며, 개인적인 목표 달성에도 도움을 줍니다. 시간을 잘 관리하는 것은 우선순위를 설정하고, 중요한 일에 집중하며, 여가 시간을 즐기는 데 필수적입니다. 또한, 시간 관리는 우리가 삶의 질을 향상시키고, 장기적인 목표를 달성하는 데 필수적인 요소입니다.\n",
      "1번을 선택하셨군요. 그럼 1번에 대한 답변을 드리겠습니다. 1번은 \"시간 관리의 중요성\"입니다. 시간 관리는 우리 일상에서 매우 중요한 역할을 합니다. 효과적인 시간 관리는 생산성을 높이고, 스트레스를 줄이며, 개인적인 목표 달성에도 도움을 줍니다. 시간을 잘 관리하는 것은 우선순위를 설정하고, 중요한 일에 집중하며, 여가 시간을 즐기는 데 필수적입니다. 또한, 시간 관리는 우리가 삶의 질을 향상시키고, 장기적인 목표를 달성하는 데 필수적인 요소입니다.\n",
      "\n",
      "=== 대화 내역 ===\n",
      "User: 오늘 하루에 대한 일기를 쓰려고 해. \n",
      "----------------------\n",
      "AI: 물론이죠! 오늘 하루에 대한 일기를 쓰는 데 도움을 드리겠습니다. 아래는 오늘 하루를 되돌아보며 답변할 수 있는 객관식 질문입니다.\n",
      "\n",
      "### 질문 1\n",
      "오늘 하루를 시작하기 전에 가장 기대했던 일은 무엇이었나요?\n",
      "\n",
      "A) 친구와의 만남\n",
      "B) 새로운 프로젝트 시작\n",
      "C) 취미 활동\n",
      "D) 휴식 시간\n",
      "\n",
      "### 질문 2\n",
      "오늘 하루 중 가장 기쁜 순간은 언제였나요?\n",
      "\n",
      "A) 아침 커피 시간\n",
      "B) 점심 식사 후의 산책\n",
      "C) 저녁 식사 시간\n",
      "D) 취미 활동 중\n",
      "\n",
      "### 질문 3\n",
      "오늘 하루 중 어려움을 겪었던 순간은 언제였나요?\n",
      "\n",
      "A) 아침 출근 시간\n",
      "B) 점심 시간\n",
      "C) 오후 업무 시간\n",
      "D) 저녁 시간\n",
      "\n",
      "### 질문 4\n",
      "오늘 하루를 통해 배운 가장 중요한 교훈은 무엇인가요?\n",
      "\n",
      "A) 시간 관리의 중요성\n",
      "B) 건강한 식습관의 중요성\n",
      "C) 사람들과의 관계 유지의 중요성\n",
      "D) 자기 자신에 대한 이해의 중요성\n",
      "\n",
      "### 질문 5\n",
      "오늘 하루를 마무리하며 내일을 위해 \n",
      "----------------------\n",
      "User: 1번을 선택할래. \n",
      "----------------------\n",
      "AI: 1번을 선택하셨군요. 그럼 1번에 대한 답변을 드리겠습니다. 1번은 \"시간 관리의 중요성\"입니다. 시간 관리는 우리 일상에서 매우 중요한 역할을 합니다. 효과적인 시간 관리는 생산성을 높이고, 스트레스를 줄이며, 개인적인 목표 달성에도 도움을 줍니다. 시간을 잘 관리하는 것은 우선순위를 설정하고, 중요한 일에 집중하며, 여가 시간을 즐기는 데 필수적입니다. 또한, 시간 관리는 우리가 삶의 질을 향상시키고, 장기적인 목표를 달성하는 데 필수적인 요소입니다. \n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer\n",
    "\n",
    "# 모델 로드\n",
    "MODEL_DIR = \"nlpai-lab/KULLM3\"\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_DIR, torch_dtype=torch.float16).to(\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "# pad_token 설정 (경고 방지)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 대화 기록 저장\n",
    "chat_history = []\n",
    "MAX_HISTORY_LENGTH = 5  # 최대 대화 저장 개수\n",
    "\n",
    "# 시스템 프롬프트 설정\n",
    "system_instruction = \"\"\"\n",
    "당신은 창의적이고 감성적인 작가입니다. 사용자가 일기를 쓸 때 도움이 되는 구체적인 객관식 질문을 제공하고,\n",
    "일기를 쉽게 작성할 수 있도록 도와주세요. 사용자가 답변을 한 경우, 그에 대한 감정을 공감하고 새로운 질문을 제공하세요.\n",
    "객관식 질문은 4개의 보기 문항을 포함하며, 새로운 질문을 연속적으로 이어가도록 하세요.\n",
    "\"\"\"\n",
    "\n",
    "def ask_kullm3(prompt):\n",
    "    \"\"\"KULLM3 모델을 사용하여 응답을 생성하고 대화 기록을 유지\"\"\"\n",
    "    global chat_history\n",
    "\n",
    "    # 기존 대화 내역과 새로운 질문을 포함한 입력 프롬프트 생성\n",
    "    messages = [{\"role\": \"system\", \"content\": system_instruction}]\n",
    "    \n",
    "    for i in range(0, len(chat_history), 2):\n",
    "        messages.append({\"role\": \"user\", \"content\": chat_history[i].replace(\"User: \", \"\")})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": chat_history[i + 1].replace(\"AI: \", \"\")})\n",
    "\n",
    "    # 새로운 사용자 입력 추가\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "    # 토크나이저 적용\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    # 모델 응답 생성\n",
    "    model.eval()\n",
    "    outputs = model.generate(inputs, streamer=streamer, max_new_tokens=512, use_cache=True)\n",
    "    response_text = tokenizer.decode(outputs[0][inputs.shape[-1]:], skip_special_tokens=True)\n",
    "\n",
    "    # 대화 기록 업데이트\n",
    "    chat_history.append(f\"User: {prompt}\")\n",
    "    chat_history.append(f\"AI: {response_text}\")\n",
    "\n",
    "    # 대화 길이 제한 (최대 저장 개수 초과 시 오래된 대화 삭제)\n",
    "    if len(chat_history) > MAX_HISTORY_LENGTH * 2:\n",
    "        chat_history = chat_history[-(MAX_HISTORY_LENGTH * 2):]\n",
    "\n",
    "    return response_text\n",
    "\n",
    "# 대화 예시\n",
    "print(ask_kullm3(\"오늘 하루에 대한 일기를 쓰려고 해.\"))\n",
    "print(ask_kullm3(\"1번을 선택할래.\"))  # 답변 반영\n",
    "\n",
    "# 대화 내역 확인\n",
    "print(\"\\n=== 대화 내역 ===\")\n",
    "for chat in chat_history:\n",
    "    print(chat, \"\\n----------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepSeek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\miniconda3\\envs\\deeplearning\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\user\\.cache\\huggingface\\hub\\models--deepseek-ai--deepseek-r1-distill-qwen-7b. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading shards: 100%|██████████| 2/2 [06:02<00:00, 181.36s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.15s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, the user wants to write a daily journal and needs two objective questions to help. I should come up with questions that encourage reflection and creativity. Let me think about topics that are meaningful and can inspire the user.\n",
      "\n",
      "First, the purpose of the world is a big question. It makes the user think about their own impact and the bigger picture. It's deep but important.\n",
      "\n",
      "Next, reflecting on the past three days can help them assess their mood and identify patterns. It's a practical approach to personal growth.\n",
      "\n",
      "I should present these as options, so the user can choose which ones to use. Then, I'll invite them to share their answers and ask follow-up questions to keep the conversation going.\n",
      "</think>\n",
      "\n",
      "물론이에요! 일기를 쓰는 데 도움이 될 객관식 질문 2개를 VOD (Video Object Description)로 제안해드리겠습니다:\n",
      "\n",
      "1. 오늘의 일기를 쓰기 위해 가장 중요한 점은 무엇인가요?  \n",
      "2. 오늘의 일기를 쓰기 위해 가장 힘든 점은 무엇인가요?  \n",
      "\n",
      "이 Questions를 VOD로 보여드리겠습니다.  \n",
      "물론이에요! 일기를 쓰기 위해 가장 중요한 점은 무엇인가요?  \n",
      "1. 오늘의 일기를 쓰기 위해 가장 중요한 점은 무엇인가요?  \n",
      "2. 오늘의 일기를 쓰기 위해 가장 힘든 점은 무엇인가요?  \n",
      "\n",
      "이 Questions를 VOD로 보여드리겠습니다.\n",
      "Alright, the user has asked for help writing a daily journal. They've already received two objective questions, and they've chosen the first one. I need to acknowledge their choice and then provide a follow-up question.\n",
      "\n",
      "First, I should express that I agree with their choice, maybe by saying I agree that the first question is important. Then, I need to come up with the next question. Since the first question was about the most important point for writing the journal, the next one could focus on the hardest part. \n",
      "\n",
      "I should make sure the questions are clear and helpful. Maybe something like asking about the most challenging aspect of writing the journal. I'll phrase it in a way that's easy to understand and encourages thoughtful reflection.\n",
      "\n",
      "I also need to keep the tone positive and supportive, encouraging the user to think deeply about their journaling process. \n",
      "\n",
      "So, putting it all together, I'll agree with their choice and then ask about the hardest part of writing the journal, making sure it's phrased naturally and clearly.\n",
      "</think>\n",
      "\n",
      "물론이에요! 1번에 선택했어. 이에 대한 감정을 공감하고 Next Question를 제시해드리겠습니다:\n",
      "\n",
      "2. 오늘의 일기를 쓰기 위해 가장 힘든 점은 무엇인가요?\n",
      "\n",
      "=== 대화 내역 ===\n",
      "User: 오늘 하루에 대한 일기를 쓰려고 해. 일기를 쓸 때 도움이 될 객관식 문제 2개 작성해줘. \n",
      "----------------------\n",
      "DeepSeek: Okay, the user wants to write a daily journal and needs two objective questions to help. I should come up with questions that encourage reflection and creativity. Let me think about topics that are meaningful and can inspire the user.\n",
      "\n",
      "First, the purpose of the world is a big question. It makes the user think about their own impact and the bigger picture. It's deep but important.\n",
      "\n",
      "Next, reflecting on the past three days can help them assess their mood and identify patterns. It's a practical approach to personal growth.\n",
      "\n",
      "I should present these as options, so the user can choose which ones to use. Then, I'll invite them to share their answers and ask follow-up questions to keep the conversation going.\n",
      "</think>\n",
      "\n",
      "물론이에요! 일기를 쓰는 데 도움이 될 객관식 질문 2개를 VOD (Video Object Description)로 제안해드리겠습니다:\n",
      "\n",
      "1. 오늘의 일기를 쓰기 위해 가장 중요한 점은 무엇인가요?  \n",
      "2. 오늘의 일기를 쓰기 위해 가장 힘든 점은 무엇인가요?  \n",
      "\n",
      "이 Questions를 VOD로 보여드리겠습니다.  \n",
      "물론이에요! 일기를 쓰기 위해 가장 중요한 점은 무엇인가요?  \n",
      "1. 오늘의 일기를 쓰기 위해 가장 중요한 점은 무엇인가요?  \n",
      "2. 오늘의 일기를 쓰기 위해 가장 힘든 점은 무엇인가요?  \n",
      "\n",
      "이 Questions를 VOD로 보여드리겠습니다. \n",
      "----------------------\n",
      "User: 1번을 선택했어. 이에 대한 감정을 공감하면서, 다음 질문을 제시해줘. \n",
      "----------------------\n",
      "DeepSeek: Alright, the user has asked for help writing a daily journal. They've already received two objective questions, and they've chosen the first one. I need to acknowledge their choice and then provide a follow-up question.\n",
      "\n",
      "First, I should express that I agree with their choice, maybe by saying I agree that the first question is important. Then, I need to come up with the next question. Since the first question was about the most important point for writing the journal, the next one could focus on the hardest part. \n",
      "\n",
      "I should make sure the questions are clear and helpful. Maybe something like asking about the most challenging aspect of writing the journal. I'll phrase it in a way that's easy to understand and encourages thoughtful reflection.\n",
      "\n",
      "I also need to keep the tone positive and supportive, encouraging the user to think deeply about their journaling process. \n",
      "\n",
      "So, putting it all together, I'll agree with their choice and then ask about the hardest part of writing the journal, making sure it's phrased naturally and clearly.\n",
      "</think>\n",
      "\n",
      "물론이에요! 1번에 선택했어. 이에 대한 감정을 공감하고 Next Question를 제시해드리겠습니다:\n",
      "\n",
      "2. 오늘의 일기를 쓰기 위해 가장 힘든 점은 무엇인가요? \n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# 모델 설정\n",
    "model_name = \"deepseek-ai/deepseek-r1-distill-qwen-7b\"\n",
    "\n",
    "# 토크나이저 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 모델 로드 (float16 또는 bfloat16 사용 가능)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,  # 커스텀 코드 실행 허용\n",
    "    torch_dtype=torch.bfloat16,  # FP8 대신 bfloat16 사용 (VRAM 절약 가능)\n",
    "    device_map=\"auto\"  # 자동으로 GPU/CPU 설정\n",
    ")\n",
    "\n",
    "# pad_token 문제 해결\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 시스템 프롬프트 설정\n",
    "system_instruction = \"\"\"당신은 창의적이고 감성적인 작가입니다. \n",
    "사용자가 일기를 쓸 때 도움이 되는 구체적인 객관식 질문을 제공하고, \n",
    "일기를 쉽게 작성할 수 있도록 도와주세요. \n",
    "사용자가 답변을 한 경우, 그에 대한 감정을 공감하고 새로운 질문을 제공하세요. \n",
    "객관식 질문은 4개의 보기 문항을 포함하며, 새로운 질문을 연속적으로 이어가도록 하세요.\n",
    "\"\"\"\n",
    "\n",
    "# 대화 기록 저장\n",
    "chat_history = []\n",
    "MAX_HISTORY_LENGTH = 5  # 최대 대화 기록 저장 개수\n",
    "\n",
    "def ask_deepseek(prompt):\n",
    "    \"\"\"DeepSeek-R1 모델을 사용하여 응답을 생성하고 대화 기록을 유지\"\"\"\n",
    "    global chat_history\n",
    "\n",
    "    # 기존 대화 내역과 새로운 질문을 포함한 입력 프롬프트 생성\n",
    "    messages = [{\"role\": \"system\", \"content\": system_instruction}]  # 시스템 메시지 추가\n",
    "    \n",
    "    for i in range(0, len(chat_history), 2):\n",
    "        messages.append({\"role\": \"user\", \"content\": chat_history[i].replace(\"User: \", \"\")})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": chat_history[i + 1].replace(\"DeepSeek: \", \"\")})\n",
    "    \n",
    "    # 사용자의 새 질문 추가\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "    # 토큰 변환\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "\n",
    "    # 종료 토큰 설정\n",
    "    eos_token = tokenizer.eos_token_id\n",
    "\n",
    "    # 모델 응답 생성\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=512,\n",
    "        eos_token_id=eos_token,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9\n",
    "    )\n",
    "\n",
    "    # 응답 디코딩\n",
    "    response_text = tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True)\n",
    "\n",
    "    # 새로운 대화 기록 추가\n",
    "    chat_history.append(f\"User: {prompt}\")\n",
    "    chat_history.append(f\"DeepSeek: {response_text}\")\n",
    "\n",
    "    # 대화 길이 제한 (최대 저장 개수 초과 시 오래된 대화 삭제)\n",
    "    if len(chat_history) > MAX_HISTORY_LENGTH * 2:\n",
    "        chat_history = chat_history[-(MAX_HISTORY_LENGTH * 2):]\n",
    "\n",
    "    return response_text\n",
    "\n",
    "# 대화 예시\n",
    "print(ask_deepseek(\"오늘 하루에 대한 일기를 쓰려고 해. 일기를 쓸 때 도움이 될 객관식 문제 2개 작성해줘.\"))\n",
    "\n",
    "# 사용자가 선택했을 때의 응답 테스트\n",
    "print(ask_deepseek(\"1번을 선택했어. 이에 대한 감정을 공감하면서, 다음 질문을 제시해줘.\"))  # 답변 반영\n",
    "\n",
    "# 대화 내역 확인\n",
    "print(\"\\n=== 대화 내역 ===\")\n",
    "for chat in chat_history:\n",
    "    print(chat, \"\\n----------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama VARCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\miniconda3\\envs\\deeplearning\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:32<00:00,  8.13s/it]\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "네, 기꺼이 도와드리겠습니다. \n",
      "\n",
      "1. 오늘 하루에 가장 기분 좋은 순간은 언제였나요?\n",
      "   A. 아침 일어나서\n",
      "   B. 아침 커피를 마시던 시기\n",
      "   C. 업무 중에\n",
      "   D. 저녁 식사 후\n",
      "\n",
      "2. 오늘 하루에 가장 힘들었던 순간은 언제였나요?\n",
      "   A. 출근하는 길에\n",
      "   B. 중요한 회의 중에\n",
      "   C. 점심시간에\n",
      "   D. 퇴근 후\n",
      "\n",
      "이제 당신이 선택한 답변에 대해 이야기해주실 수 있으실까요? 가장 기분 좋은 순간과 가장 힘들었던 순간에 대해 간단히 적어보시면 좋을 것 같아요. \n",
      "\n",
      "그 다음에 새로운 질문을 준비해드리겠습니다:\n",
      "1. 기분 좋은 순간에서 느꼈던 감정은 무엇이었나요? \n",
      "   A. 행복\n",
      "   B. 안정감\n",
      "   C. 만족감\n",
      "   D. 기쁨\n",
      "\n",
      "2. 힘들었던 순간에서 겪었던 감정은 무엇이었나요?\n",
      "   A. 스트레스\n",
      "   B. 좌절감\n",
      "   C. 불안\n",
      "   D. 분노\n",
      "\n",
      "당신의 답변을 기다리겠습니다.\n",
      "아, 아침 일어나서 기분 좋았셨군요. 새벽의 평화와 고요함, 그리고 하루를 시작하는 활력이 느껴지실 거예요. \n",
      "\n",
      "그런 순간에는 일상의 소소한 기쁨에 감사하게 느끼시죠? \n",
      "\n",
      "다음 질문을 제시해 보겠습니다:\n",
      "\n",
      "1. 오늘 아침에 가장 기분 좋게 느꼈던 이유는 무엇이었나요?\n",
      "   A. 좋은 잠을 자서\n",
      "   B. 좋아하는 음악을 들으며\n",
      "   C. 창문을 통해 들어온 따뜻한 햇살\n",
      "   D. 아침 식사를 맛있게 먹었기 때문\n",
      "\n",
      "당신의 선택에 대해 적어주시면 그에 맞는 새로운 질문을 준비해 드리겠습니다.\n",
      "\n",
      "=== 대화 내역 ===\n",
      "User: 오늘 하루에 대한 일기를 쓰려고 해. 일기를 쓸 때 도움이 될 객관식 문제 2개 작성해줘. \n",
      "----------------------\n",
      "DeepSeek: 네, 기꺼이 도와드리겠습니다. \n",
      "\n",
      "1. 오늘 하루에 가장 기분 좋은 순간은 언제였나요?\n",
      "   A. 아침 일어나서\n",
      "   B. 아침 커피를 마시던 시기\n",
      "   C. 업무 중에\n",
      "   D. 저녁 식사 후\n",
      "\n",
      "2. 오늘 하루에 가장 힘들었던 순간은 언제였나요?\n",
      "   A. 출근하는 길에\n",
      "   B. 중요한 회의 중에\n",
      "   C. 점심시간에\n",
      "   D. 퇴근 후\n",
      "\n",
      "이제 당신이 선택한 답변에 대해 이야기해주실 수 있으실까요? 가장 기분 좋은 순간과 가장 힘들었던 순간에 대해 간단히 적어보시면 좋을 것 같아요. \n",
      "\n",
      "그 다음에 새로운 질문을 준비해드리겠습니다:\n",
      "1. 기분 좋은 순간에서 느꼈던 감정은 무엇이었나요? \n",
      "   A. 행복\n",
      "   B. 안정감\n",
      "   C. 만족감\n",
      "   D. 기쁨\n",
      "\n",
      "2. 힘들었던 순간에서 겪었던 감정은 무엇이었나요?\n",
      "   A. 스트레스\n",
      "   B. 좌절감\n",
      "   C. 불안\n",
      "   D. 분노\n",
      "\n",
      "당신의 답변을 기다리겠습니다. \n",
      "----------------------\n",
      "User: 1번을 선택했어. 이에 대한 감정을 공감하면서, 다음 질문을 제시해줘. \n",
      "----------------------\n",
      "DeepSeek: 아, 아침 일어나서 기분 좋았셨군요. 새벽의 평화와 고요함, 그리고 하루를 시작하는 활력이 느껴지실 거예요. \n",
      "\n",
      "그런 순간에는 일상의 소소한 기쁨에 감사하게 느끼시죠? \n",
      "\n",
      "다음 질문을 제시해 보겠습니다:\n",
      "\n",
      "1. 오늘 아침에 가장 기분 좋게 느꼈던 이유는 무엇이었나요?\n",
      "   A. 좋은 잠을 자서\n",
      "   B. 좋아하는 음악을 들으며\n",
      "   C. 창문을 통해 들어온 따뜻한 햇살\n",
      "   D. 아침 식사를 맛있게 먹었기 때문\n",
      "\n",
      "당신의 선택에 대해 적어주시면 그에 맞는 새로운 질문을 준비해 드리겠습니다. \n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "model_id = \"NCSOFT/Llama-VARCO-8B-Instruct\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map=\"auto\")\n",
    "\n",
    "# pad_token 문제 해결\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 시스템 프롬프트 설정\n",
    "system_instruction = \"\"\"당신은 창의적이고 감성적인 작가입니다. \n",
    "사용자가 일기를 쓸 때 도움이 되는 구체적인 객관식 질문을 제공하고, \n",
    "일기를 쉽게 작성할 수 있도록 도와주세요. \n",
    "사용자가 답변을 한 경우, 그에 대한 감정을 공감하고 새로운 질문을 제공하세요. \n",
    "객관식 질문은 4개의 보기 문항을 포함하며, 새로운 질문을 연속적으로 이어가도록 하세요.\n",
    "\"\"\"\n",
    "\n",
    "# 대화 기록 저장\n",
    "chat_history = []\n",
    "MAX_HISTORY_LENGTH = 5  # 최대 대화 기록 저장 개수\n",
    "\n",
    "def ask_deepseek(prompt):\n",
    "    \"\"\"DeepSeek-R1 모델을 사용하여 응답을 생성하고 대화 기록을 유지\"\"\"\n",
    "    global chat_history\n",
    "\n",
    "    # 기존 대화 내역과 새로운 질문을 포함한 입력 프롬프트 생성\n",
    "    messages = [{\"role\": \"system\", \"content\": system_instruction}]  # 시스템 메시지 추가\n",
    "    \n",
    "    for i in range(0, len(chat_history), 2):\n",
    "        messages.append({\"role\": \"user\", \"content\": chat_history[i].replace(\"User: \", \"\")})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": chat_history[i + 1].replace(\"DeepSeek: \", \"\")})\n",
    "    \n",
    "    # 사용자의 새 질문 추가\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "    # 토큰 변환\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "\n",
    "    # 종료 토큰 설정\n",
    "    eos_token = tokenizer.eos_token_id\n",
    "\n",
    "    # 모델 응답 생성\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=512,\n",
    "        eos_token_id=eos_token,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9\n",
    "    )\n",
    "\n",
    "    # 응답 디코딩\n",
    "    response_text = tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True)\n",
    "\n",
    "    # 새로운 대화 기록 추가\n",
    "    chat_history.append(f\"User: {prompt}\")\n",
    "    chat_history.append(f\"DeepSeek: {response_text}\")\n",
    "\n",
    "    # 대화 길이 제한 (최대 저장 개수 초과 시 오래된 대화 삭제)\n",
    "    if len(chat_history) > MAX_HISTORY_LENGTH * 2:\n",
    "        chat_history = chat_history[-(MAX_HISTORY_LENGTH * 2):]\n",
    "\n",
    "    return response_text\n",
    "\n",
    "# 대화 예시\n",
    "print(ask_deepseek(\"오늘 하루에 대한 일기를 쓰려고 해. 일기를 쓸 때 도움이 될 객관식 문제 2개 작성해줘.\"))\n",
    "\n",
    "# 사용자가 선택했을 때의 응답 테스트\n",
    "print(ask_deepseek(\"1번을 선택했어. 이에 대한 감정을 공감하면서, 다음 질문을 제시해줘.\"))  # 답변 반영\n",
    "\n",
    "# 대화 내역 확인\n",
    "print(\"\\n=== 대화 내역 ===\")\n",
    "for chat in chat_history:\n",
    "    print(chat, \"\\n----------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
