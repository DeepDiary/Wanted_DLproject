{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>아 진짜! 사무실에서 피지 말라니깐! 간접흡연이 얼마나 안좋은데!</td>\n",
       "      <td>분노</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>그럼 직접흡연하는 난 얼마나 안좋겠니? 안그래? 보면 꼭... 지 생각만 하고.</td>\n",
       "      <td>혐오</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>손님 왔어요.</td>\n",
       "      <td>중립</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>손님? 누구?</td>\n",
       "      <td>중립</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>몰라요. 팀장님 친구래요.</td>\n",
       "      <td>중립</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94189</th>\n",
       "      <td>솔직히 예보 제대로 못하는 데 세금이라도 아끼게 그냥 폐지해라..</td>\n",
       "      <td>혐오</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94190</th>\n",
       "      <td>재미가 없으니 망하지</td>\n",
       "      <td>혐오</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94191</th>\n",
       "      <td>공장 도시락 비우생적임 아르바이트했는데 화장실가성 손도 않씯고 재료 담고 바닥 떨어...</td>\n",
       "      <td>혐오</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94192</th>\n",
       "      <td>코딱지 만한 나라에서 지들끼리 피터지게 싸우는 센징 클래스 ㅉㅉㅉ</td>\n",
       "      <td>혐오</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94193</th>\n",
       "      <td>와이프도 그렇고 댓글 다 볼텐데 이휘재 좀 하차 하라고 전해주세요</td>\n",
       "      <td>혐오</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>94194 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Sentence Emotion\n",
       "0                   아 진짜! 사무실에서 피지 말라니깐! 간접흡연이 얼마나 안좋은데!      분노\n",
       "1           그럼 직접흡연하는 난 얼마나 안좋겠니? 안그래? 보면 꼭... 지 생각만 하고.      혐오\n",
       "2                                                손님 왔어요.      중립\n",
       "3                                                손님? 누구?      중립\n",
       "4                                         몰라요. 팀장님 친구래요.      중립\n",
       "...                                                  ...     ...\n",
       "94189               솔직히 예보 제대로 못하는 데 세금이라도 아끼게 그냥 폐지해라..      혐오\n",
       "94190                                        재미가 없으니 망하지      혐오\n",
       "94191  공장 도시락 비우생적임 아르바이트했는데 화장실가성 손도 않씯고 재료 담고 바닥 떨어...      혐오\n",
       "94192               코딱지 만한 나라에서 지들끼리 피터지게 싸우는 센징 클래스 ㅉㅉㅉ      혐오\n",
       "94193               와이프도 그렇고 댓글 다 볼텐데 이휘재 좀 하차 하라고 전해주세요      혐오\n",
       "\n",
       "[94194 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"ai_hub_data.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "model = AutoModel.from_pretrained(\"monologg/kobert\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"monologg/kobert\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "data['Sentence'] = data[\"Sentence\"].apply(lambda x: re.sub(\"[^0-9a-zA-Z가-힣\\s+]\", \"\", x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Sentence, Emotion]\n",
       "Index: []"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data[\"Emotion\"] == 5].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# label 수치화\n",
    "def encoding(label):\n",
    "    if label == \"중립\":\n",
    "        return 0\n",
    "    elif label == \"놀람\":\n",
    "        return 1\n",
    "    elif label == \"분노\":\n",
    "        return 2\n",
    "    elif label == \"슬픔\":\n",
    "        return 3\n",
    "    elif label == \"행복\":\n",
    "        return 4\n",
    "    elif label == \"혐오\":\n",
    "        return 5\n",
    "    elif label == \"공포\":\n",
    "        return 6\n",
    "\n",
    "data['Emotion'] = data['Emotion'].apply(lambda x: encoding(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터셋 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainX: (75355,), TrainY: (75355,)\n",
      "TestX: (18839,), TestY: (18839,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "trainX, testX, trainY, testY = train_test_split(\n",
    "    data[\"Sentence\"],\n",
    "    data[\"Emotion\"],\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"TrainX: {trainX.shape}, TrainY: {trainY.shape}\")\n",
    "print(f\"TestX: {testX.shape}, TestY: {testY.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainX: (67819,), TrainY: (67819,)\n",
      "ValX: (7536,), ValY: (7536,)\n",
      "TestX: (18839,), TestY: (18839,)\n"
     ]
    }
   ],
   "source": [
    "trainX, valX, trainY, valY = train_test_split(\n",
    "    trainX, trainY, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"TrainX: {trainX.shape}, TrainY: {trainY.shape}\")\n",
    "print(f\"ValX: {valX.shape}, ValY: {valY.shape}\")\n",
    "print(f\"TestX: {testX.shape}, TestY: {testY.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 토크나이징 클래스화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts.tolist() \n",
    "        self.labels = labels.tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        encoded_output = self.tokenizer(\n",
    "            text, \n",
    "            max_length = self.max_length, # 모델이 처리할 수 있는 최대 토큰의 길이\n",
    "            padding=\"max_length\", # max_length 보다 짧을 경우 패딩 토큰을 추가하여 길이를 맞춘다\n",
    "            truncation=True, # max_length 보다 길경우 자른다\n",
    "            add_special_tokens=True, # bert 모델의 필요한 특수토큰을 자동으로 추가(?)\n",
    "            return_token_type_ids=True, # 텍스트를 변환해서 반환할거냐 ?\n",
    "            return_attention_mask=True, # 실제토큰 1 패딩토큰 0 반환\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return encoded_output\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 데이터 가져오기\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # 토크나이징\n",
    "        encoding = self.tokenize(text)\n",
    "\n",
    "        # 라벨 추가\n",
    "        # encoding[\"label\"] = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].flatten(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
    "            \"token_type_ids\": encoding[\"token_type_ids\"].flatten(),\n",
    "            \"label\": torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 128\n",
    "train_dataset = CustomDataset(trainX, trainY, tokenizer, max_length)\n",
    "val_dataset = CustomDataset(valX, valY, tokenizer, max_length)\n",
    "test_dataset = CustomDataset(testX, testY, tokenizer, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size = batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size = batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel\n",
    "\n",
    "class CustomKoBERT(nn.Module): # pyTorch의 nn.Module을 상속하여 클래스 정의\n",
    "    def __init__(self, num_labels):\n",
    "        super(CustomKoBERT, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(\"monologg/kobert\")\n",
    "        self.classifier = nn.Linear(768, num_labels)  #분류 헤드 정의: BERT의 출력(768차원)을 입력으로, 분류할 클래스 개수만큼출력생성하는 nn.linear정의\n",
    "        self.loss_fn = nn.CrossEntropyLoss()  # 손실 함수 정의\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, labels=None):\n",
    "        # BERT 모델의 출력\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            token_type_ids=token_type_ids)\n",
    "        pooled_output = outputs[1]  # KoBERT 의 두번째 값인 pooled_output을 가져옴 이것은? -> 문장의 전체의미를 요약한 벡터\n",
    "        \n",
    "        # 분류 헤드 통과\n",
    "        logits = self.classifier(pooled_output)  # 전체의미를요약한 벡터를 분류기에 넣어서 분류결과를 생성\n",
    "        \n",
    "        # 손실 계산 (labels가 주어진 경우)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = self.loss_fn(logits, labels) # 예측값과 라벨과 손실 값을 계산\n",
    "\n",
    "        return {\"loss\": loss, \"logits\": logits} # train 중에는 손실 값 사용 / 추론 시 불류 결과 사용\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, Train_loss: 1.5669022231257446, Validation_loss: 1.5391176285384314\n",
      "epoch: 1, Train_loss: 1.5539898996332477, Validation_loss: 1.5386065797061677\n",
      "epoch: 2, Train_loss: 1.55131397014025, Validation_loss: 1.5367664109496302\n",
      "epoch: 3, Train_loss: 1.5507040051998127, Validation_loss: 1.552088091454435\n",
      "epoch: 4, Train_loss: 1.550665170867015, Validation_loss: 1.5464408539409598\n",
      "epoch: 5, Train_loss: 1.5502346398075668, Validation_loss: 1.5364124277594742\n",
      "epoch: 6, Train_loss: 1.5498483987001432, Validation_loss: 1.537480770149555\n",
      "epoch: 7, Train_loss: 1.549864774445041, Validation_loss: 1.5395106650461816\n",
      "epoch: 8, Train_loss: 1.5499083122671118, Validation_loss: 1.5375366218530448\n",
      "epoch: 9, Train_loss: 1.5493453188558002, Validation_loss: 1.535996670176269\n",
      "epoch: 10, Train_loss: 1.5492845488254818, Validation_loss: 1.5363697475435374\n",
      "epoch: 11, Train_loss: 1.549496886361733, Validation_loss: 1.538023228161907\n",
      "epoch: 12, Train_loss: 1.5492230918357908, Validation_loss: 1.5365477313155076\n",
      "epoch: 13, Train_loss: 1.5487996030344937, Validation_loss: 1.5401548616698846\n",
      "epoch: 14, Train_loss: 1.5488271637251443, Validation_loss: 1.5392744099511715\n",
      "epoch: 15, Train_loss: 1.548848696179257, Validation_loss: 1.5392222687317307\n",
      "epoch: 16, Train_loss: 1.5475482963805904, Validation_loss: 1.539964264555342\n",
      "epoch: 17, Train_loss: 1.5477017301106848, Validation_loss: 1.5360972835126703\n",
      "epoch: 18, Train_loss: 1.5481449628382866, Validation_loss: 1.5385191001456762\n",
      "epoch: 19, Train_loss: 1.5480491993067604, Validation_loss: 1.5409531662292035\n",
      "epoch: 20, Train_loss: 1.547955096264541, Validation_loss: 1.5391676724589807\n",
      "epoch: 21, Train_loss: 1.5480559873968343, Validation_loss: 1.5370408724321174\n",
      "epoch: 22, Train_loss: 1.54667900465659, Validation_loss: 1.5526686273436132\n",
      "epoch: 23, Train_loss: 1.5454448327011485, Validation_loss: 1.5372285921355975\n",
      "epoch: 24, Train_loss: 1.5448031307073595, Validation_loss: 1.565437012733674\n",
      "epoch: 25, Train_loss: 1.5431944647946088, Validation_loss: 1.5435974886462946\n",
      "epoch: 26, Train_loss: 1.5422962144115873, Validation_loss: 1.5737349477036997\n",
      "epoch: 27, Train_loss: 1.5403914141952733, Validation_loss: 1.5657385261828227\n",
      "epoch: 28, Train_loss: 1.5402232937063367, Validation_loss: 1.5903684836664018\n",
      "epoch: 29, Train_loss: 1.5398010613880724, Validation_loss: 1.6210302004388943\n",
      "epoch: 30, Train_loss: 1.5379320128355403, Validation_loss: 1.584200608312704\n",
      "epoch: 31, Train_loss: 1.5345578004108087, Validation_loss: 1.6532023561102063\n",
      "epoch: 32, Train_loss: 1.5330320066713778, Validation_loss: 1.7134064155336146\n",
      "epoch: 33, Train_loss: 1.5312915743377258, Validation_loss: 1.752874895139358\n",
      "epoch: 34, Train_loss: 1.5297323213372893, Validation_loss: 1.7831543579602698\n",
      "epoch: 35, Train_loss: 1.529831317347904, Validation_loss: 1.7881647333534407\n",
      "epoch: 36, Train_loss: 1.5290367184991105, Validation_loss: 1.8011269204302705\n",
      "epoch: 37, Train_loss: 1.5279095592669492, Validation_loss: 1.8562182939735947\n",
      "epoch: 38, Train_loss: 1.528166671825715, Validation_loss: 1.9201899058246814\n",
      "epoch: 39, Train_loss: 1.5267254127668148, Validation_loss: 1.887363709493554\n",
      "epoch: 40, Train_loss: 1.5263302990669514, Validation_loss: 1.8789302329285635\n",
      "epoch: 41, Train_loss: 1.52609533187442, Validation_loss: 1.8989302321730652\n",
      "epoch: 42, Train_loss: 1.5259901879802962, Validation_loss: 1.923997323667421\n",
      "epoch: 43, Train_loss: 1.525337376276249, Validation_loss: 1.898969078329718\n",
      "epoch: 44, Train_loss: 1.5252925864885798, Validation_loss: 1.9172257633599\n",
      "epoch: 45, Train_loss: 1.5246919969714439, Validation_loss: 1.9350100527903078\n",
      "epoch: 46, Train_loss: 1.5247258339321397, Validation_loss: 1.9499259726510179\n",
      "epoch: 47, Train_loss: 1.5241060542347358, Validation_loss: 1.9711273273971177\n",
      "epoch: 48, Train_loss: 1.5238017048240438, Validation_loss: 1.9908860103719552\n",
      "epoch: 49, Train_loss: 1.5237113512030438, Validation_loss: 1.9898968392749754\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "# CustomKoBERT 모델 생성\n",
    "num_labels = 7\n",
    "batch_size = 8\n",
    "epochs = 50\n",
    "model = CustomKoBERT(num_labels=num_labels).to(device)\n",
    "\n",
    "# Optimizer 및 스케줄러 설정 (변경 없음)\n",
    "optimizer = AdamW(model.parameters(), lr=0.001)\n",
    "total_steps = len(train_loader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "loss_history = {\"train\": [], \"validation\" : []}\n",
    "patience = 7\n",
    "patience_cnt = 0\n",
    "best_loss_val = float('inf')\n",
    "\n",
    "# 학습 루프 (변경 없음)\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    loss_train = 0.0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        token_type_ids = batch['token_type_ids'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass (loss와 logits 반환)\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        loss = outputs[\"loss\"]\n",
    "        loss.backward()\n",
    "\n",
    "        clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        loss_train += loss.item() * batch_size\n",
    "\n",
    "    loss_history['train'].append(loss_train / len(train_dataset))\n",
    "\n",
    "    #### Validation ####\n",
    "    model.eval()\n",
    "    loss_val = 0.0\n",
    "\n",
    "    for batch in val_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        token_type_ids = batch['token_type_ids'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids,\n",
    "                labels=labels\n",
    "            )\n",
    "            loss = outputs[\"loss\"]\n",
    "\n",
    "        loss_val += loss.item() * batch_size\n",
    "\n",
    "    loss_history['validation'].append(loss_val / len(val_dataset))\n",
    "\n",
    "    #### Early Stopping ####\n",
    "    if loss_val < best_loss_val:\n",
    "        best_loss_val = loss_val\n",
    "        torch.save(model.state_dict(), \"Bert_best_model.pth\")\n",
    "        patience_cnt += 1\n",
    "        if patience_cnt == patience:\n",
    "            print(\"Early stopping!\")\n",
    "            break\n",
    "\n",
    "    print(f\"epoch: {epoch}, Train_loss: {loss_train/len(train_dataset)}, Validation_loss: {loss_val / len(val_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loss_history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mloss_history\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(loss_history[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loss_history' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(loss_history['train'])\n",
    "plt.plot(loss_history['validation'])\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1730976/76308731.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"./Bert_best_model.pth\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "model = CustomKoBERT(num_labels=7)\n",
    "model.load_state_dict(torch.load(\"./Bert_best_model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.5477738504709981, Accuracy: 0.5136684775352478\n"
     ]
    }
   ],
   "source": [
    "#### Test ####\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "loss_test = 0.0\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        # gpu 보내기\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        token_type_ids = batch['token_type_ids'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        # print(input_ids)\n",
    "        # print(attention_mask)\n",
    "        # print(token_type_ids)\n",
    "\n",
    "        # 예측\n",
    "        outputs = model(\n",
    "        input_ids = input_ids,\n",
    "        attention_mask = attention_mask,\n",
    "        token_type_ids = token_type_ids,\n",
    "        labels = labels \n",
    "        )\n",
    "        \n",
    "        loss = outputs[\"loss\"]\n",
    "        preds = outputs['logits'].argmax(dim=1)\n",
    "        correct += (labels == preds).sum()\n",
    "\n",
    "        # loss 저장\n",
    "        loss_test += loss.item() * batch_size\n",
    "print(f\"Test Loss: {loss_test / len(test_dataset)}, Accuracy: {correct / len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"monologg/kobert\", trust_remote_code=True)\n",
    "\n",
    "max_length = 128\n",
    "text = \"나는 너무 행복해\"\n",
    "\n",
    "encoded_input = tokenizer(\n",
    "    text, \n",
    "    max_length = max_length,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    add_special_tokens=True,\n",
    "    return_token_type_ids=True,\n",
    "    return_attention_mask=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "# print(encoded_output)\n",
    "model = model.to(device)\n",
    "encoded_input = encoded_input.to(device)\n",
    "outputs = model(**encoded_input)\n",
    "print(outputs['logits'].argmax(dim=1).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-24.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
